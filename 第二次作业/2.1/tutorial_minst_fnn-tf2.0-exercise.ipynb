{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, datasets\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
    "\n",
    "def mnist_dataset():\n",
    "    (x, y), (x_test, y_test) = datasets.mnist.load_data()\n",
    "    #normalize\n",
    "    x = x/255.0\n",
    "    x_test = x_test/255.0\n",
    "    \n",
    "    return (x, y), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip([1, 2, 3, 4], ['a', 'b', 'c', 'd'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myModel:\n",
    "    def __init__(self):\n",
    "        ####################\n",
    "        '''声明模型对应的参数'''\n",
    "        ####################\n",
    "        initializer = tf.keras.initializers.GlorotUniform()\n",
    "        self.W1 = tf.Variable(initializer(shape=[28 * 28, 100]), trainable=True)\n",
    "        self.b1 = tf.Variable(tf.zeros(100), trainable=True)\n",
    "        self.W2 = tf.Variable(initializer(shape=[100, 10]), trainable=True)\n",
    "        self.b2 = tf.Variable(tf.zeros(10), trainable=True)\n",
    "        self.trainable_variables = [self.W1, self.W2, self.b1, self.b2]\n",
    "    def __call__(self, x):\n",
    "        ####################\n",
    "        '''实现模型函数体，返回未归一化的logits'''\n",
    "        ####################\n",
    "        x = tf.reshape(x, [-1, 28 * 28])\n",
    "        x = tf.nn.relu(tf.matmul(x, self.W1) + self.b1)  # 改用ReLU\n",
    "        logits = tf.matmul(x, self.W2) + self.b2\n",
    "        return logits\n",
    "        \n",
    "model = myModel()\n",
    "\n",
    "optimizer = optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels))\n",
    "\n",
    "@tf.function\n",
    "def compute_accuracy(logits, labels):\n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))\n",
    "\n",
    "@tf.function\n",
    "def train_one_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = compute_loss(logits, y)\n",
    "\n",
    "    # compute gradient\n",
    "    trainable_vars = [model.W1, model.W2, model.b1, model.b2]\n",
    "    grads = tape.gradient(loss, trainable_vars)\n",
    "    for g, v in zip(grads, trainable_vars):\n",
    "        v.assign_sub(0.01*g)\n",
    "\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "\n",
    "    # loss and accuracy is scalar tensor\n",
    "    return loss, accuracy\n",
    "\n",
    "@tf.function\n",
    "def test(model, x, y):\n",
    "    logits = model(x)\n",
    "    loss = compute_loss(logits, y)\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实际训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : loss 2.3443182 ; accuracy 0.11721667\n",
      "epoch 1 : loss 2.335508 ; accuracy 0.122033335\n",
      "epoch 2 : loss 2.3268347 ; accuracy 0.12681666\n",
      "epoch 3 : loss 2.3182936 ; accuracy 0.13223334\n",
      "epoch 4 : loss 2.3098757 ; accuracy 0.13816667\n",
      "epoch 5 : loss 2.3015761 ; accuracy 0.14455\n",
      "epoch 6 : loss 2.2933855 ; accuracy 0.15088333\n",
      "epoch 7 : loss 2.2853005 ; accuracy 0.15615\n",
      "epoch 8 : loss 2.2773187 ; accuracy 0.16226667\n",
      "epoch 9 : loss 2.2694316 ; accuracy 0.16808334\n",
      "epoch 10 : loss 2.2616308 ; accuracy 0.17485\n",
      "epoch 11 : loss 2.2539163 ; accuracy 0.18135\n",
      "epoch 12 : loss 2.246285 ; accuracy 0.18745\n",
      "epoch 13 : loss 2.2387328 ; accuracy 0.19383334\n",
      "epoch 14 : loss 2.2312562 ; accuracy 0.19956666\n",
      "epoch 15 : loss 2.223851 ; accuracy 0.20613334\n",
      "epoch 16 : loss 2.2165124 ; accuracy 0.21271667\n",
      "epoch 17 : loss 2.2092352 ; accuracy 0.22011666\n",
      "epoch 18 : loss 2.2020178 ; accuracy 0.22725\n",
      "epoch 19 : loss 2.1948593 ; accuracy 0.23411667\n",
      "epoch 20 : loss 2.1877563 ; accuracy 0.24096666\n",
      "epoch 21 : loss 2.180705 ; accuracy 0.2476\n",
      "epoch 22 : loss 2.173703 ; accuracy 0.25515\n",
      "epoch 23 : loss 2.1667492 ; accuracy 0.26211667\n",
      "epoch 24 : loss 2.1598392 ; accuracy 0.26961666\n",
      "epoch 25 : loss 2.152972 ; accuracy 0.27671668\n",
      "epoch 26 : loss 2.1461444 ; accuracy 0.28385\n",
      "epoch 27 : loss 2.1393557 ; accuracy 0.29093334\n",
      "epoch 28 : loss 2.1326067 ; accuracy 0.29798332\n",
      "epoch 29 : loss 2.125896 ; accuracy 0.30508333\n",
      "epoch 30 : loss 2.1192183 ; accuracy 0.31265\n",
      "epoch 31 : loss 2.1125767 ; accuracy 0.31985\n",
      "epoch 32 : loss 2.1059678 ; accuracy 0.32705\n",
      "epoch 33 : loss 2.0993876 ; accuracy 0.33421665\n",
      "epoch 34 : loss 2.0928364 ; accuracy 0.3413\n",
      "epoch 35 : loss 2.0863132 ; accuracy 0.34835\n",
      "epoch 36 : loss 2.0798132 ; accuracy 0.3551\n",
      "epoch 37 : loss 2.0733345 ; accuracy 0.36213332\n",
      "epoch 38 : loss 2.0668778 ; accuracy 0.36818334\n",
      "epoch 39 : loss 2.0604417 ; accuracy 0.37481666\n",
      "epoch 40 : loss 2.0540268 ; accuracy 0.38116667\n",
      "epoch 41 : loss 2.0476341 ; accuracy 0.38736665\n",
      "epoch 42 : loss 2.04126 ; accuracy 0.39348334\n",
      "epoch 43 : loss 2.034904 ; accuracy 0.39885\n",
      "epoch 44 : loss 2.028564 ; accuracy 0.40465\n",
      "epoch 45 : loss 2.0222416 ; accuracy 0.41048333\n",
      "epoch 46 : loss 2.0159364 ; accuracy 0.41611665\n",
      "epoch 47 : loss 2.0096455 ; accuracy 0.42183334\n",
      "epoch 48 : loss 2.0033681 ; accuracy 0.4271\n",
      "epoch 49 : loss 1.9971055 ; accuracy 0.43236667\n",
      "epoch 50 : loss 1.9908553 ; accuracy 0.43838334\n",
      "epoch 51 : loss 1.9846172 ; accuracy 0.44396666\n",
      "epoch 52 : loss 1.9783914 ; accuracy 0.44905\n",
      "epoch 53 : loss 1.9721777 ; accuracy 0.45413333\n",
      "epoch 54 : loss 1.9659766 ; accuracy 0.45948333\n",
      "epoch 55 : loss 1.959788 ; accuracy 0.46453333\n",
      "epoch 56 : loss 1.9536114 ; accuracy 0.46935\n",
      "epoch 57 : loss 1.9474463 ; accuracy 0.47428334\n",
      "epoch 58 : loss 1.9412918 ; accuracy 0.47906667\n",
      "epoch 59 : loss 1.9351479 ; accuracy 0.4836\n",
      "epoch 60 : loss 1.9290135 ; accuracy 0.48786667\n",
      "epoch 61 : loss 1.9228901 ; accuracy 0.4925\n",
      "epoch 62 : loss 1.9167771 ; accuracy 0.49673334\n",
      "epoch 63 : loss 1.9106755 ; accuracy 0.50098336\n",
      "epoch 64 : loss 1.9045846 ; accuracy 0.5054333\n",
      "epoch 65 : loss 1.8985053 ; accuracy 0.51015\n",
      "epoch 66 : loss 1.8924375 ; accuracy 0.514\n",
      "epoch 67 : loss 1.8863803 ; accuracy 0.51855\n",
      "epoch 68 : loss 1.880332 ; accuracy 0.5225833\n",
      "epoch 69 : loss 1.8742945 ; accuracy 0.52645\n",
      "epoch 70 : loss 1.8682685 ; accuracy 0.53036666\n",
      "epoch 71 : loss 1.8622526 ; accuracy 0.53426665\n",
      "epoch 72 : loss 1.8562468 ; accuracy 0.53845\n",
      "epoch 73 : loss 1.85025 ; accuracy 0.54258335\n",
      "epoch 74 : loss 1.8442651 ; accuracy 0.54645\n",
      "epoch 75 : loss 1.8382909 ; accuracy 0.5495667\n",
      "epoch 76 : loss 1.8323293 ; accuracy 0.5528167\n",
      "epoch 77 : loss 1.8263804 ; accuracy 0.55626667\n",
      "epoch 78 : loss 1.8204437 ; accuracy 0.5598\n",
      "epoch 79 : loss 1.8145193 ; accuracy 0.5632833\n",
      "epoch 80 : loss 1.8086067 ; accuracy 0.56665\n",
      "epoch 81 : loss 1.802706 ; accuracy 0.5703667\n",
      "epoch 82 : loss 1.7968185 ; accuracy 0.5735667\n",
      "epoch 83 : loss 1.7909462 ; accuracy 0.57696664\n",
      "epoch 84 : loss 1.7850868 ; accuracy 0.5800667\n",
      "epoch 85 : loss 1.7792406 ; accuracy 0.58316666\n",
      "epoch 86 : loss 1.7734091 ; accuracy 0.5858667\n",
      "epoch 87 : loss 1.7675911 ; accuracy 0.58863336\n",
      "epoch 88 : loss 1.7617873 ; accuracy 0.59173334\n",
      "epoch 89 : loss 1.7559974 ; accuracy 0.59461665\n",
      "epoch 90 : loss 1.7502224 ; accuracy 0.5974333\n",
      "epoch 91 : loss 1.7444607 ; accuracy 0.60025\n",
      "epoch 92 : loss 1.7387125 ; accuracy 0.60285\n",
      "epoch 93 : loss 1.7329779 ; accuracy 0.6052\n",
      "epoch 94 : loss 1.7272581 ; accuracy 0.6080667\n",
      "epoch 95 : loss 1.7215546 ; accuracy 0.6108\n",
      "epoch 96 : loss 1.7158669 ; accuracy 0.61335\n",
      "epoch 97 : loss 1.7101935 ; accuracy 0.61583334\n",
      "epoch 98 : loss 1.7045346 ; accuracy 0.6178833\n",
      "epoch 99 : loss 1.6988925 ; accuracy 0.6202833\n",
      "epoch 100 : loss 1.6932658 ; accuracy 0.62225\n",
      "epoch 101 : loss 1.6876564 ; accuracy 0.62493336\n",
      "epoch 102 : loss 1.6820626 ; accuracy 0.6274167\n",
      "epoch 103 : loss 1.676485 ; accuracy 0.62985\n",
      "epoch 104 : loss 1.6709237 ; accuracy 0.6318\n",
      "epoch 105 : loss 1.6653789 ; accuracy 0.63413334\n",
      "epoch 106 : loss 1.6598513 ; accuracy 0.6367667\n",
      "epoch 107 : loss 1.6543412 ; accuracy 0.63893336\n",
      "epoch 108 : loss 1.6488495 ; accuracy 0.64136666\n",
      "epoch 109 : loss 1.6433764 ; accuracy 0.6433\n",
      "epoch 110 : loss 1.6379216 ; accuracy 0.64488333\n",
      "epoch 111 : loss 1.6324852 ; accuracy 0.6465667\n",
      "epoch 112 : loss 1.627067 ; accuracy 0.64851665\n",
      "epoch 113 : loss 1.6216673 ; accuracy 0.6503\n",
      "epoch 114 : loss 1.6162862 ; accuracy 0.652\n",
      "epoch 115 : loss 1.6109244 ; accuracy 0.65386665\n",
      "epoch 116 : loss 1.6055808 ; accuracy 0.6558833\n",
      "epoch 117 : loss 1.6002567 ; accuracy 0.65745\n",
      "epoch 118 : loss 1.5949507 ; accuracy 0.65891665\n",
      "epoch 119 : loss 1.5896627 ; accuracy 0.6605\n",
      "epoch 120 : loss 1.5843935 ; accuracy 0.66205\n",
      "epoch 121 : loss 1.5791426 ; accuracy 0.6635\n",
      "epoch 122 : loss 1.5739107 ; accuracy 0.6653\n",
      "epoch 123 : loss 1.5686985 ; accuracy 0.6669833\n",
      "epoch 124 : loss 1.5635065 ; accuracy 0.6685333\n",
      "epoch 125 : loss 1.5583342 ; accuracy 0.6703\n",
      "epoch 126 : loss 1.5531826 ; accuracy 0.67195\n",
      "epoch 127 : loss 1.5480515 ; accuracy 0.6734\n",
      "epoch 128 : loss 1.5429409 ; accuracy 0.6751\n",
      "epoch 129 : loss 1.5378506 ; accuracy 0.67653334\n",
      "epoch 130 : loss 1.5327808 ; accuracy 0.67793334\n",
      "epoch 131 : loss 1.5277314 ; accuracy 0.67925\n",
      "epoch 132 : loss 1.522703 ; accuracy 0.68075\n",
      "epoch 133 : loss 1.517695 ; accuracy 0.6823\n",
      "epoch 134 : loss 1.512708 ; accuracy 0.68348336\n",
      "epoch 135 : loss 1.5077416 ; accuracy 0.68481666\n",
      "epoch 136 : loss 1.5027964 ; accuracy 0.68598336\n",
      "epoch 137 : loss 1.497872 ; accuracy 0.6871833\n",
      "epoch 138 : loss 1.4929677 ; accuracy 0.68846667\n",
      "epoch 139 : loss 1.4880849 ; accuracy 0.68981665\n",
      "epoch 140 : loss 1.4832226 ; accuracy 0.69091666\n",
      "epoch 141 : loss 1.478382 ; accuracy 0.6916\n",
      "epoch 142 : loss 1.4735618 ; accuracy 0.69266665\n",
      "epoch 143 : loss 1.4687626 ; accuracy 0.69405\n",
      "epoch 144 : loss 1.4639852 ; accuracy 0.695\n",
      "epoch 145 : loss 1.45923 ; accuracy 0.69593334\n",
      "epoch 146 : loss 1.4544955 ; accuracy 0.69733334\n",
      "epoch 147 : loss 1.4497819 ; accuracy 0.6985\n",
      "epoch 148 : loss 1.4450893 ; accuracy 0.69981664\n",
      "epoch 149 : loss 1.4404185 ; accuracy 0.70086664\n",
      "epoch 150 : loss 1.4357696 ; accuracy 0.70203334\n",
      "epoch 151 : loss 1.431142 ; accuracy 0.70311666\n",
      "epoch 152 : loss 1.4265364 ; accuracy 0.70393336\n",
      "epoch 153 : loss 1.4219531 ; accuracy 0.70523334\n",
      "epoch 154 : loss 1.4173919 ; accuracy 0.70626664\n",
      "epoch 155 : loss 1.412852 ; accuracy 0.7072167\n",
      "epoch 156 : loss 1.4083343 ; accuracy 0.70816666\n",
      "epoch 157 : loss 1.4038384 ; accuracy 0.7091333\n",
      "epoch 158 : loss 1.3993638 ; accuracy 0.71015\n",
      "epoch 159 : loss 1.3949114 ; accuracy 0.71106666\n",
      "epoch 160 : loss 1.3904802 ; accuracy 0.71205\n",
      "epoch 161 : loss 1.3860704 ; accuracy 0.713\n",
      "epoch 162 : loss 1.3816823 ; accuracy 0.7140833\n",
      "epoch 163 : loss 1.3773161 ; accuracy 0.7151167\n",
      "epoch 164 : loss 1.3729715 ; accuracy 0.71605\n",
      "epoch 165 : loss 1.3686479 ; accuracy 0.7169\n",
      "epoch 166 : loss 1.3643457 ; accuracy 0.7179667\n",
      "epoch 167 : loss 1.3600653 ; accuracy 0.7187833\n",
      "epoch 168 : loss 1.3558062 ; accuracy 0.71971667\n",
      "epoch 169 : loss 1.3515681 ; accuracy 0.7206333\n",
      "epoch 170 : loss 1.3473511 ; accuracy 0.72138333\n",
      "epoch 171 : loss 1.3431559 ; accuracy 0.72216666\n",
      "epoch 172 : loss 1.3389819 ; accuracy 0.72333336\n",
      "epoch 173 : loss 1.3348292 ; accuracy 0.72393334\n",
      "epoch 174 : loss 1.3306977 ; accuracy 0.72506666\n",
      "epoch 175 : loss 1.3265882 ; accuracy 0.72606665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 176 : loss 1.3224998 ; accuracy 0.72715\n",
      "epoch 177 : loss 1.3184326 ; accuracy 0.72775\n",
      "epoch 178 : loss 1.3143867 ; accuracy 0.72828335\n",
      "epoch 179 : loss 1.3103628 ; accuracy 0.7287667\n",
      "epoch 180 : loss 1.3063601 ; accuracy 0.7294\n",
      "epoch 181 : loss 1.3023787 ; accuracy 0.7302\n",
      "epoch 182 : loss 1.2984184 ; accuracy 0.7309833\n",
      "epoch 183 : loss 1.2944791 ; accuracy 0.73178333\n",
      "epoch 184 : loss 1.2905611 ; accuracy 0.7325\n",
      "epoch 185 : loss 1.2866644 ; accuracy 0.73328334\n",
      "epoch 186 : loss 1.2827888 ; accuracy 0.7341833\n",
      "epoch 187 : loss 1.2789338 ; accuracy 0.7349833\n",
      "epoch 188 : loss 1.2750987 ; accuracy 0.7356333\n",
      "epoch 189 : loss 1.2712841 ; accuracy 0.7363833\n",
      "epoch 190 : loss 1.26749 ; accuracy 0.73698336\n",
      "epoch 191 : loss 1.2637165 ; accuracy 0.73793334\n",
      "epoch 192 : loss 1.2599635 ; accuracy 0.73866665\n",
      "epoch 193 : loss 1.2562307 ; accuracy 0.7391833\n",
      "epoch 194 : loss 1.2525175 ; accuracy 0.73983335\n",
      "epoch 195 : loss 1.2488244 ; accuracy 0.74058336\n",
      "epoch 196 : loss 1.2451515 ; accuracy 0.74148333\n",
      "epoch 197 : loss 1.2414987 ; accuracy 0.74226665\n",
      "epoch 198 : loss 1.2378656 ; accuracy 0.74308336\n",
      "epoch 199 : loss 1.2342522 ; accuracy 0.7438833\n",
      "epoch 200 : loss 1.2306589 ; accuracy 0.74476665\n",
      "epoch 201 : loss 1.2270852 ; accuracy 0.74535\n",
      "epoch 202 : loss 1.2235314 ; accuracy 0.74623334\n",
      "epoch 203 : loss 1.2199974 ; accuracy 0.74686664\n",
      "epoch 204 : loss 1.2164825 ; accuracy 0.7474\n",
      "epoch 205 : loss 1.212987 ; accuracy 0.7481\n",
      "epoch 206 : loss 1.2095102 ; accuracy 0.74878335\n",
      "epoch 207 : loss 1.2060524 ; accuracy 0.7496\n",
      "epoch 208 : loss 1.2026137 ; accuracy 0.7502\n",
      "epoch 209 : loss 1.1991936 ; accuracy 0.7509\n",
      "epoch 210 : loss 1.1957922 ; accuracy 0.75158334\n",
      "epoch 211 : loss 1.19241 ; accuracy 0.75226665\n",
      "epoch 212 : loss 1.1890469 ; accuracy 0.7529167\n",
      "epoch 213 : loss 1.1857018 ; accuracy 0.7536333\n",
      "epoch 214 : loss 1.182375 ; accuracy 0.75445\n",
      "epoch 215 : loss 1.1790664 ; accuracy 0.75505\n",
      "epoch 216 : loss 1.1757761 ; accuracy 0.75591666\n",
      "epoch 217 : loss 1.1725048 ; accuracy 0.75645\n",
      "epoch 218 : loss 1.1692518 ; accuracy 0.75706667\n",
      "epoch 219 : loss 1.1660167 ; accuracy 0.7579333\n",
      "epoch 220 : loss 1.1628003 ; accuracy 0.75843334\n",
      "epoch 221 : loss 1.1596023 ; accuracy 0.75913334\n",
      "epoch 222 : loss 1.156422 ; accuracy 0.7597333\n",
      "epoch 223 : loss 1.1532592 ; accuracy 0.7601\n",
      "epoch 224 : loss 1.1501136 ; accuracy 0.7607667\n",
      "epoch 225 : loss 1.1469859 ; accuracy 0.76135\n",
      "epoch 226 : loss 1.1438752 ; accuracy 0.7618833\n",
      "epoch 227 : loss 1.1407821 ; accuracy 0.7621833\n",
      "epoch 228 : loss 1.1377063 ; accuracy 0.7627\n",
      "epoch 229 : loss 1.1346477 ; accuracy 0.76346666\n",
      "epoch 230 : loss 1.1316065 ; accuracy 0.76393336\n",
      "epoch 231 : loss 1.128582 ; accuracy 0.7646\n",
      "epoch 232 : loss 1.1255745 ; accuracy 0.7651\n",
      "epoch 233 : loss 1.1225836 ; accuracy 0.76556665\n",
      "epoch 234 : loss 1.1196091 ; accuracy 0.7662167\n",
      "epoch 235 : loss 1.1166515 ; accuracy 0.76668334\n",
      "epoch 236 : loss 1.1137103 ; accuracy 0.76705\n",
      "epoch 237 : loss 1.1107857 ; accuracy 0.76746666\n",
      "epoch 238 : loss 1.1078777 ; accuracy 0.7680167\n",
      "epoch 239 : loss 1.104986 ; accuracy 0.7686833\n",
      "epoch 240 : loss 1.1021106 ; accuracy 0.769\n",
      "epoch 241 : loss 1.0992514 ; accuracy 0.7694833\n",
      "epoch 242 : loss 1.0964082 ; accuracy 0.76998335\n",
      "epoch 243 : loss 1.0935808 ; accuracy 0.77045\n",
      "epoch 244 : loss 1.0907696 ; accuracy 0.77095\n",
      "epoch 245 : loss 1.0879749 ; accuracy 0.77143335\n",
      "epoch 246 : loss 1.085196 ; accuracy 0.77203333\n",
      "epoch 247 : loss 1.0824329 ; accuracy 0.7726\n",
      "epoch 248 : loss 1.0796849 ; accuracy 0.7729333\n",
      "epoch 249 : loss 1.0769522 ; accuracy 0.7733333\n",
      "epoch 250 : loss 1.0742346 ; accuracy 0.77388334\n",
      "epoch 251 : loss 1.0715322 ; accuracy 0.77455\n",
      "epoch 252 : loss 1.0688447 ; accuracy 0.7751833\n",
      "epoch 253 : loss 1.0661722 ; accuracy 0.7758333\n",
      "epoch 254 : loss 1.0635144 ; accuracy 0.7763\n",
      "epoch 255 : loss 1.0608714 ; accuracy 0.7766\n",
      "epoch 256 : loss 1.0582432 ; accuracy 0.77701664\n",
      "epoch 257 : loss 1.0556293 ; accuracy 0.77748334\n",
      "epoch 258 : loss 1.0530304 ; accuracy 0.77805\n",
      "epoch 259 : loss 1.0504462 ; accuracy 0.77828336\n",
      "epoch 260 : loss 1.0478765 ; accuracy 0.7788333\n",
      "epoch 261 : loss 1.045321 ; accuracy 0.77923334\n",
      "epoch 262 : loss 1.0427802 ; accuracy 0.77965\n",
      "epoch 263 : loss 1.0402536 ; accuracy 0.78026664\n",
      "epoch 264 : loss 1.0377407 ; accuracy 0.78076667\n",
      "epoch 265 : loss 1.0352418 ; accuracy 0.78115\n",
      "epoch 266 : loss 1.0327567 ; accuracy 0.7815167\n",
      "epoch 267 : loss 1.0302851 ; accuracy 0.7819167\n",
      "epoch 268 : loss 1.0278274 ; accuracy 0.7822833\n",
      "epoch 269 : loss 1.0253834 ; accuracy 0.7826833\n",
      "epoch 270 : loss 1.022953 ; accuracy 0.78305\n",
      "epoch 271 : loss 1.0205361 ; accuracy 0.78345\n",
      "epoch 272 : loss 1.0181324 ; accuracy 0.78393334\n",
      "epoch 273 : loss 1.015742 ; accuracy 0.78433335\n",
      "epoch 274 : loss 1.0133646 ; accuracy 0.78466666\n",
      "epoch 275 : loss 1.0110006 ; accuracy 0.78496665\n",
      "epoch 276 : loss 1.0086497 ; accuracy 0.78533334\n",
      "epoch 277 : loss 1.006312 ; accuracy 0.78578335\n",
      "epoch 278 : loss 1.0039871 ; accuracy 0.7862167\n",
      "epoch 279 : loss 1.0016748 ; accuracy 0.7865833\n",
      "epoch 280 : loss 0.9993754 ; accuracy 0.78695\n",
      "epoch 281 : loss 0.99708843 ; accuracy 0.7874333\n",
      "epoch 282 : loss 0.99481404 ; accuracy 0.78785\n",
      "epoch 283 : loss 0.992552 ; accuracy 0.78818333\n",
      "epoch 284 : loss 0.9903022 ; accuracy 0.7886\n",
      "epoch 285 : loss 0.9880644 ; accuracy 0.78886664\n",
      "epoch 286 : loss 0.98583865 ; accuracy 0.78908336\n",
      "epoch 287 : loss 0.98362494 ; accuracy 0.78933334\n",
      "epoch 288 : loss 0.98142284 ; accuracy 0.7898\n",
      "epoch 289 : loss 0.9792327 ; accuracy 0.7902167\n",
      "epoch 290 : loss 0.9770543 ; accuracy 0.79055\n",
      "epoch 291 : loss 0.9748876 ; accuracy 0.79083335\n",
      "epoch 292 : loss 0.97273296 ; accuracy 0.79105\n",
      "epoch 293 : loss 0.97058934 ; accuracy 0.7913667\n",
      "epoch 294 : loss 0.96845704 ; accuracy 0.7917167\n",
      "epoch 295 : loss 0.9663364 ; accuracy 0.79218334\n",
      "epoch 296 : loss 0.9642273 ; accuracy 0.79265\n",
      "epoch 297 : loss 0.9621299 ; accuracy 0.79316664\n",
      "epoch 298 : loss 0.9600439 ; accuracy 0.7935\n",
      "epoch 299 : loss 0.95796907 ; accuracy 0.7938167\n",
      "epoch 300 : loss 0.9559053 ; accuracy 0.7941333\n",
      "epoch 301 : loss 0.9538526 ; accuracy 0.7943\n",
      "epoch 302 : loss 0.9518106 ; accuracy 0.7945667\n",
      "epoch 303 : loss 0.9497795 ; accuracy 0.79476666\n",
      "epoch 304 : loss 0.9477595 ; accuracy 0.79505\n",
      "epoch 305 : loss 0.9457501 ; accuracy 0.79535\n",
      "epoch 306 : loss 0.94375175 ; accuracy 0.79551667\n",
      "epoch 307 : loss 0.94176406 ; accuracy 0.79581666\n",
      "epoch 308 : loss 0.9397871 ; accuracy 0.79616666\n",
      "epoch 309 : loss 0.93782073 ; accuracy 0.79651666\n",
      "epoch 310 : loss 0.93586445 ; accuracy 0.79685\n",
      "epoch 311 : loss 0.93391854 ; accuracy 0.7972\n",
      "epoch 312 : loss 0.93198305 ; accuracy 0.7975\n",
      "epoch 313 : loss 0.93005776 ; accuracy 0.7977167\n",
      "epoch 314 : loss 0.9281426 ; accuracy 0.79801667\n",
      "epoch 315 : loss 0.9262375 ; accuracy 0.79855\n",
      "epoch 316 : loss 0.92434245 ; accuracy 0.7987667\n",
      "epoch 317 : loss 0.92245734 ; accuracy 0.79908335\n",
      "epoch 318 : loss 0.92058206 ; accuracy 0.79933333\n",
      "epoch 319 : loss 0.91871655 ; accuracy 0.7996167\n",
      "epoch 320 : loss 0.91686064 ; accuracy 0.79995\n",
      "epoch 321 : loss 0.9150142 ; accuracy 0.8002167\n",
      "epoch 322 : loss 0.91317767 ; accuracy 0.80041665\n",
      "epoch 323 : loss 0.9113504 ; accuracy 0.80075\n",
      "epoch 324 : loss 0.909533 ; accuracy 0.80105\n",
      "epoch 325 : loss 0.90772486 ; accuracy 0.8013\n",
      "epoch 326 : loss 0.90592605 ; accuracy 0.8016833\n",
      "epoch 327 : loss 0.9041365 ; accuracy 0.8020167\n",
      "epoch 328 : loss 0.9023564 ; accuracy 0.8024\n",
      "epoch 329 : loss 0.9005853 ; accuracy 0.80261666\n",
      "epoch 330 : loss 0.8988233 ; accuracy 0.803\n",
      "epoch 331 : loss 0.8970706 ; accuracy 0.8033\n",
      "epoch 332 : loss 0.8953268 ; accuracy 0.8035\n",
      "epoch 333 : loss 0.89359194 ; accuracy 0.8038667\n",
      "epoch 334 : loss 0.89186585 ; accuracy 0.8042\n",
      "epoch 335 : loss 0.8901483 ; accuracy 0.80441666\n",
      "epoch 336 : loss 0.8884399 ; accuracy 0.80473334\n",
      "epoch 337 : loss 0.8867402 ; accuracy 0.8050333\n",
      "epoch 338 : loss 0.88504916 ; accuracy 0.80541664\n",
      "epoch 339 : loss 0.8833669 ; accuracy 0.8057\n",
      "epoch 340 : loss 0.8816933 ; accuracy 0.80605\n",
      "epoch 341 : loss 0.8800282 ; accuracy 0.80625\n",
      "epoch 342 : loss 0.8783716 ; accuracy 0.80651665\n",
      "epoch 343 : loss 0.8767234 ; accuracy 0.8067667\n",
      "epoch 344 : loss 0.87508374 ; accuracy 0.80715\n",
      "epoch 345 : loss 0.87345237 ; accuracy 0.8074333\n",
      "epoch 346 : loss 0.87182915 ; accuracy 0.80775\n",
      "epoch 347 : loss 0.87021416 ; accuracy 0.80806667\n",
      "epoch 348 : loss 0.8686076 ; accuracy 0.80836666\n",
      "epoch 349 : loss 0.86700904 ; accuracy 0.8086333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 350 : loss 0.8654184 ; accuracy 0.8089\n",
      "epoch 351 : loss 0.8638357 ; accuracy 0.8092167\n",
      "epoch 352 : loss 0.86226106 ; accuracy 0.80946666\n",
      "epoch 353 : loss 0.86069465 ; accuracy 0.80978334\n",
      "epoch 354 : loss 0.85913587 ; accuracy 0.80986667\n",
      "epoch 355 : loss 0.857585 ; accuracy 0.81005\n",
      "epoch 356 : loss 0.8560418 ; accuracy 0.81035\n",
      "epoch 357 : loss 0.8545065 ; accuracy 0.81065\n",
      "epoch 358 : loss 0.85297877 ; accuracy 0.81075\n",
      "epoch 359 : loss 0.8514587 ; accuracy 0.81091666\n",
      "epoch 360 : loss 0.8499462 ; accuracy 0.8111333\n",
      "epoch 361 : loss 0.848441 ; accuracy 0.8114167\n",
      "epoch 362 : loss 0.8469431 ; accuracy 0.81168336\n",
      "epoch 363 : loss 0.84545237 ; accuracy 0.8118\n",
      "epoch 364 : loss 0.84396875 ; accuracy 0.81205\n",
      "epoch 365 : loss 0.8424926 ; accuracy 0.81233335\n",
      "epoch 366 : loss 0.84102345 ; accuracy 0.8125833\n",
      "epoch 367 : loss 0.8395616 ; accuracy 0.8127667\n",
      "epoch 368 : loss 0.83810675 ; accuracy 0.8131\n",
      "epoch 369 : loss 0.83665913 ; accuracy 0.81335\n",
      "epoch 370 : loss 0.83521837 ; accuracy 0.8136\n",
      "epoch 371 : loss 0.8337845 ; accuracy 0.8138\n",
      "epoch 372 : loss 0.8323579 ; accuracy 0.814\n",
      "epoch 373 : loss 0.8309383 ; accuracy 0.81418335\n",
      "epoch 374 : loss 0.82952535 ; accuracy 0.8143333\n",
      "epoch 375 : loss 0.82811916 ; accuracy 0.81455\n",
      "epoch 376 : loss 0.8267196 ; accuracy 0.81478333\n",
      "epoch 377 : loss 0.8253267 ; accuracy 0.8150833\n",
      "epoch 378 : loss 0.82394034 ; accuracy 0.81526667\n",
      "epoch 379 : loss 0.82256055 ; accuracy 0.8154\n",
      "epoch 380 : loss 0.82118726 ; accuracy 0.81561667\n",
      "epoch 381 : loss 0.8198204 ; accuracy 0.81583333\n",
      "epoch 382 : loss 0.81845987 ; accuracy 0.8160667\n",
      "epoch 383 : loss 0.81710565 ; accuracy 0.81621665\n",
      "epoch 384 : loss 0.8157577 ; accuracy 0.8164833\n",
      "epoch 385 : loss 0.814416 ; accuracy 0.81675\n",
      "epoch 386 : loss 0.8130807 ; accuracy 0.81691664\n",
      "epoch 387 : loss 0.8117516 ; accuracy 0.8171333\n",
      "epoch 388 : loss 0.81042874 ; accuracy 0.8173\n",
      "epoch 389 : loss 0.8091121 ; accuracy 0.8174667\n",
      "epoch 390 : loss 0.80780154 ; accuracy 0.81768334\n",
      "epoch 391 : loss 0.8064972 ; accuracy 0.8179333\n",
      "epoch 392 : loss 0.8051989 ; accuracy 0.8181\n",
      "epoch 393 : loss 0.8039065 ; accuracy 0.81848335\n",
      "epoch 394 : loss 0.8026198 ; accuracy 0.81876665\n",
      "epoch 395 : loss 0.8013391 ; accuracy 0.8189667\n",
      "epoch 396 : loss 0.80006397 ; accuracy 0.8192\n",
      "epoch 397 : loss 0.79879487 ; accuracy 0.81941664\n",
      "epoch 398 : loss 0.7975315 ; accuracy 0.8196\n",
      "epoch 399 : loss 0.79627407 ; accuracy 0.81986666\n",
      "epoch 400 : loss 0.79502213 ; accuracy 0.8201333\n",
      "epoch 401 : loss 0.7937761 ; accuracy 0.8203667\n",
      "epoch 402 : loss 0.79253554 ; accuracy 0.8207\n",
      "epoch 403 : loss 0.79130083 ; accuracy 0.8208\n",
      "epoch 404 : loss 0.7900719 ; accuracy 0.8211\n",
      "epoch 405 : loss 0.78884816 ; accuracy 0.82133335\n",
      "epoch 406 : loss 0.7876301 ; accuracy 0.82165\n",
      "epoch 407 : loss 0.7864173 ; accuracy 0.82198334\n",
      "epoch 408 : loss 0.78521013 ; accuracy 0.8221833\n",
      "epoch 409 : loss 0.7840083 ; accuracy 0.82235\n",
      "epoch 410 : loss 0.78281206 ; accuracy 0.8226333\n",
      "epoch 411 : loss 0.7816211 ; accuracy 0.8228667\n",
      "epoch 412 : loss 0.7804355 ; accuracy 0.82305\n",
      "epoch 413 : loss 0.7792552 ; accuracy 0.82318336\n",
      "epoch 414 : loss 0.7780801 ; accuracy 0.8233167\n",
      "epoch 415 : loss 0.7769102 ; accuracy 0.8235\n",
      "epoch 416 : loss 0.77574545 ; accuracy 0.82376665\n",
      "epoch 417 : loss 0.77458596 ; accuracy 0.82386667\n",
      "epoch 418 : loss 0.77343166 ; accuracy 0.82406664\n",
      "epoch 419 : loss 0.77228236 ; accuracy 0.82428336\n",
      "epoch 420 : loss 0.7711383 ; accuracy 0.82441664\n",
      "epoch 421 : loss 0.76999897 ; accuracy 0.82455\n",
      "epoch 422 : loss 0.7688647 ; accuracy 0.8247333\n",
      "epoch 423 : loss 0.7677353 ; accuracy 0.8249\n",
      "epoch 424 : loss 0.76661074 ; accuracy 0.82515\n",
      "epoch 425 : loss 0.7654911 ; accuracy 0.8253667\n",
      "epoch 426 : loss 0.7643763 ; accuracy 0.82558334\n",
      "epoch 427 : loss 0.76326627 ; accuracy 0.8257833\n",
      "epoch 428 : loss 0.7621611 ; accuracy 0.82593334\n",
      "epoch 429 : loss 0.76106083 ; accuracy 0.82615\n",
      "epoch 430 : loss 0.75996536 ; accuracy 0.82636666\n",
      "epoch 431 : loss 0.7588746 ; accuracy 0.8264833\n",
      "epoch 432 : loss 0.75778854 ; accuracy 0.8265833\n",
      "epoch 433 : loss 0.75670713 ; accuracy 0.8268167\n",
      "epoch 434 : loss 0.7556303 ; accuracy 0.82711667\n",
      "epoch 435 : loss 0.7545582 ; accuracy 0.82731664\n",
      "epoch 436 : loss 0.75349075 ; accuracy 0.8275167\n",
      "epoch 437 : loss 0.75242794 ; accuracy 0.82771665\n",
      "epoch 438 : loss 0.75136966 ; accuracy 0.8279\n",
      "epoch 439 : loss 0.750316 ; accuracy 0.8281\n",
      "epoch 440 : loss 0.7492669 ; accuracy 0.8283333\n",
      "epoch 441 : loss 0.7482224 ; accuracy 0.8286\n",
      "epoch 442 : loss 0.7471822 ; accuracy 0.82883334\n",
      "epoch 443 : loss 0.7461465 ; accuracy 0.8289833\n",
      "epoch 444 : loss 0.7451151 ; accuracy 0.82913333\n",
      "epoch 445 : loss 0.7440882 ; accuracy 0.8293\n",
      "epoch 446 : loss 0.7430656 ; accuracy 0.82946664\n",
      "epoch 447 : loss 0.7420473 ; accuracy 0.82963336\n",
      "epoch 448 : loss 0.7410334 ; accuracy 0.82981664\n",
      "epoch 449 : loss 0.7400239 ; accuracy 0.82993335\n",
      "epoch 450 : loss 0.7390185 ; accuracy 0.83031666\n",
      "epoch 451 : loss 0.7380173 ; accuracy 0.8304333\n",
      "epoch 452 : loss 0.7370203 ; accuracy 0.83066666\n",
      "epoch 453 : loss 0.7360276 ; accuracy 0.8308333\n",
      "epoch 454 : loss 0.7350389 ; accuracy 0.83096665\n",
      "epoch 455 : loss 0.73405415 ; accuracy 0.8311667\n",
      "epoch 456 : loss 0.7330736 ; accuracy 0.8313\n",
      "epoch 457 : loss 0.73209685 ; accuracy 0.8315167\n",
      "epoch 458 : loss 0.7311242 ; accuracy 0.8316333\n",
      "epoch 459 : loss 0.7301556 ; accuracy 0.83175\n",
      "epoch 460 : loss 0.7291909 ; accuracy 0.8318167\n",
      "epoch 461 : loss 0.72823024 ; accuracy 0.83195\n",
      "epoch 462 : loss 0.7272733 ; accuracy 0.83215\n",
      "epoch 463 : loss 0.72632045 ; accuracy 0.83231664\n",
      "epoch 464 : loss 0.72537136 ; accuracy 0.83245\n",
      "epoch 465 : loss 0.72442615 ; accuracy 0.8327\n",
      "epoch 466 : loss 0.7234847 ; accuracy 0.83285\n",
      "epoch 467 : loss 0.722547 ; accuracy 0.8329167\n",
      "epoch 468 : loss 0.7216131 ; accuracy 0.8330167\n",
      "epoch 469 : loss 0.7206829 ; accuracy 0.83313334\n",
      "epoch 470 : loss 0.71975666 ; accuracy 0.83335\n",
      "epoch 471 : loss 0.71883416 ; accuracy 0.83355\n",
      "epoch 472 : loss 0.7179154 ; accuracy 0.83365\n",
      "epoch 473 : loss 0.71700037 ; accuracy 0.83393335\n",
      "epoch 474 : loss 0.71608907 ; accuracy 0.83405\n",
      "epoch 475 : loss 0.71518135 ; accuracy 0.83418334\n",
      "epoch 476 : loss 0.7142774 ; accuracy 0.8343833\n",
      "epoch 477 : loss 0.71337706 ; accuracy 0.83453333\n",
      "epoch 478 : loss 0.7124804 ; accuracy 0.8347167\n",
      "epoch 479 : loss 0.71158725 ; accuracy 0.83491665\n",
      "epoch 480 : loss 0.71069753 ; accuracy 0.8351\n",
      "epoch 481 : loss 0.70981133 ; accuracy 0.83528334\n",
      "epoch 482 : loss 0.70892864 ; accuracy 0.8354333\n",
      "epoch 483 : loss 0.7080494 ; accuracy 0.8355833\n",
      "epoch 484 : loss 0.7071736 ; accuracy 0.83571666\n",
      "epoch 485 : loss 0.70630133 ; accuracy 0.8359333\n",
      "epoch 486 : loss 0.7054324 ; accuracy 0.8361\n",
      "epoch 487 : loss 0.70456684 ; accuracy 0.83633333\n",
      "epoch 488 : loss 0.7037046 ; accuracy 0.83646667\n",
      "epoch 489 : loss 0.7028457 ; accuracy 0.8365833\n",
      "epoch 490 : loss 0.70198995 ; accuracy 0.83675\n",
      "epoch 491 : loss 0.7011376 ; accuracy 0.83683336\n",
      "epoch 492 : loss 0.7002886 ; accuracy 0.8369833\n",
      "epoch 493 : loss 0.6994429 ; accuracy 0.83715\n",
      "epoch 494 : loss 0.6986004 ; accuracy 0.83725\n",
      "epoch 495 : loss 0.69776106 ; accuracy 0.83741665\n",
      "epoch 496 : loss 0.69692504 ; accuracy 0.8376833\n",
      "epoch 497 : loss 0.6960922 ; accuracy 0.83786666\n",
      "epoch 498 : loss 0.6952626 ; accuracy 0.8379667\n",
      "epoch 499 : loss 0.6944363 ; accuracy 0.83811665\n",
      "test loss 0.67058164 ; accuracy 0.8436\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = mnist_dataset()\n",
    "for epoch in range(500):\n",
    "    loss, accuracy = train_one_step(model, optimizer, \n",
    "                                    tf.constant(train_data[0], dtype=tf.float32), \n",
    "                                    tf.constant(train_data[1], dtype=tf.int64))\n",
    "    print('epoch', epoch, ': loss', loss.numpy(), '; accuracy', accuracy.numpy())\n",
    "loss, accuracy = test(model, \n",
    "                      tf.constant(test_data[0], dtype=tf.float32), \n",
    "                      tf.constant(test_data[1], dtype=tf.int64))\n",
    "\n",
    "print('test loss', loss.numpy(), '; accuracy', accuracy.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
