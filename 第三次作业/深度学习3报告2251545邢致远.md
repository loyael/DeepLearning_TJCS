## 深度学习3：卷积神经网络生成唐诗报告

**作者：2251545 邢致远**

---

###  1.RNN，LSTM与GRU模型

#### 1.1 RNN模型

循环神经网络（Recurrent Neural Network，RNN）是一种用于处理序列数据的神经网络。相比一般的神经网络来说，他能够处理序列变化的数据。比如某个单词的意思会因为上文提到的内容不同而有不同的含义，RNN就能够很好地解决这类问题。

![img](https://pic2.zhimg.com/v2-f716c816d46792b867a6815c278f11cb_r.jpg)



 上图中*x*为当前状态下数据的输入， *h* 表示接收到的上一个节点的输入。*y*为当前节点状态下的输出，而 *h'* 为传递到下一个节点的输出。

通常输出 *h'* 与 *x* 和 *h* 的值都相关；而 *y* 则常常使用 *h'*投入到一个线性层（主要是进行维度映射）然后使用softmax进行分类得到需要的数据。

通过序列形式的输入，能够得到如下形式的RNN。

![img](https://pic4.zhimg.com/v2-71652d6a1eee9def631c18ea5e3c7605_r.jpg)

#### 1.2 LSTM模型

##### 1.2.1 LSTM概述

长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。

![img](https://pic4.zhimg.com/v2-e4f9851cad426dfe4ab1c76209546827_r.jpg)

相比RNN只有一个传递状态 *h^t^* ，LSTM有两个传输状态，一个 *c^t^* ，和一个 *h^t^* （其实RNN中的 *h^t^* 对于LSTM中的 *c^t^* ）。其中 *c^t^* 通常改变得很慢，是上一个状态传过来的 *c^t−1^* 加上一些数值。而 *h^t^* 则在不同节点下往往会有很大的区别。

##### 1.2.2 LSTM推理过程

首先使用LSTM的当前输入 *x^t^* 和上一个状态传递下来的 *h^t−1^* 拼接训练得到四个状态。

![img](https://picx.zhimg.com/v2-15c5eb554f843ec492579c6d87e1497b_r.jpg)

![img](https://pic1.zhimg.com/v2-d044fd0087e1df5d2a1089b441db9970_r.jpg)

其中 *z^f^ ， z^i^ ，z^o^* 是由拼接向量乘以权重矩阵之后，再通过一个 sigmoid 激活函数转换成0到1之间的数值，来作为一种门控状态。而 z 则是将结果通过一个 tanh 激活函数将转换成-1到1之间的值（这里使用 tanh 是因为这里是将其做为输入数据，而不是门控信号）。

![img](https://pic4.zhimg.com/v2-556c74f0e025a47fea05dc0f76ea775d_r.jpg)

**输出的产生分为以下三个阶段：**

* 忘记阶段。这个阶段主要是对上一个节点传进来的输入进行选择性忘记。简单来说就是会 “忘记不重要的，记住重要的”。具体来说是通过计算得到的 *z^f^* 来作为忘记门控，来控制上一个状态的 *c^t−1^* 哪些需要留哪些需要忘。

* 选择记忆阶段。这个阶段将这个阶段的输入有选择性地进行“记忆”。主要是会对输入 *x^t^* 进行选择记忆。哪些重要则着重记录下来，哪些不重要，则少记一些。当前的输入内容由前面计算得到的 *z* 表示。而选择的门控信号则是由 *z^i^* 来进行控制。
* 输出阶段。这个阶段将决定哪些将会被当成当前状态的输出。主要是通过 *z^o^* 来进行控制的。并且还对上一阶段得到的 *c^o^* 进行了放缩。
* 与普通RNN类似，输出 yt 往往最终也是通过 ht 变化得到。

**综上所述**，LSTM通过门控状态来控制传输状态，记住需要长时间记忆的，忘记不重要的信息；而不像普通的RNN那样只能机械地记忆叠加。但也因为引入了很多内容，导致参数变多，也使得训练难度加大了很多。

#### 1.3 GRU模型

##### 1.3.1 GRU概述

GRU（Gate Recurrent Unit）是循环神经网络RNN的一种。和LSTM一样，也是为了解决长期记忆和反向传播中的梯度等问题而提出来的。相比LSTM，使用GRU能够达到相当的效果，并且相比之下更容易进行训练，能够很大程度上提高训练效率，因此很多时候会更倾向于使用GRU。

![image-20250328212922949](C:\Users\邢致远\AppData\Roaming\Typora\typora-user-images\image-20250328212922949.png)

##### 1.3.2 GRU推理过程

首先，我们先通过上一个传输下来的状态 *h^t−1^* 和当前节点的输入 *x^t^* 来获取两个门控状态。其中 r 控制重置的门控（reset gate）， z 为控制更新的门控（update gate）。

![img](https://pic1.zhimg.com/v2-7fff5d817530dada1b279c7279d73b8a_r.jpg)

![img](https://pic2.zhimg.com/v2-390781506bbebbef799f1a12acd7865b_r.jpg)

得到门控信号之后，首先使用重置门控来得到“重置”之后的数据 *h^{t−1}′^=h^t−1^⊙r* ，再将 *h^t−1′^* 与输入 *x^t^* 进行拼接，再通过一个tanh激活函数来将数据放缩到-1~1的范围内,得到*h‘*。这里的 *h′* 主要是包含了当前输入的 *x^t^* 数据。有针对性地对 *h′* 添加到当前的隐藏状态，相当于”记忆了当前时刻的状态“。

![image-20250328214253703](C:\Users\邢致远\AppData\Roaming\Typora\typora-user-images\image-20250328214253703.png)

最后，使用同一个门控 z 同时进行遗忘和选择记忆：

- *(1−z)⊙h^t−1^* ：表示对原本隐藏状态的选择性“遗忘”。这里的 *1−z* 可以想象成遗忘门（forget gate），忘记 *h^t−1^* 维度中一些不重要的信息。
- *z⊙h′* ： 表示对包含当前节点信息的 *h′* 进行选择性”记忆“。与上面类似，这里的 (1−z) 同理会忘记 h′ 维度中的一些不重要的信息。或者，这里我们更应当看做是对 h′ 维度中的某些信息进行选择。
- *h^t^=(1−z)⊙h^t−1^+z⊙h′* ：结合上述，这一步的操作就是忘记传递下来的 ht−1 中的某些维度信息，并加入当前节点输入的某些维度信息。

GRU输入输出的结构与普通的RNN相似，其中的内部思想与LSTM相似。

**综上所述**，与LSTM相比，GRU内部少了一个”门控“，参数比LSTM少，但是却也能够达到与LSTM相当的功能。



### 2.诗词生成过程

一、数据处理阶段（main.py）

- 读取数据集后过滤含特殊符号的文本
- 添加开始符G和结束符E
- 按长度排序并建立字符索引映射表

- 向量化转换，将诗歌转换为数字序列

```python
# process_poems1()核心逻辑
content = start_token + content + end_token  # 添加开始/结束符 [G...E]
all_words = [word for word in poem]  # 字符级分割
word_int_map = dict(zip(words, range(len(words))))  # 建立字符索引映射
poems_vector = [list(map(word_int_map.get, poem))]  # 转化为数字序列
```

二、模型结构（rnn.py）

```python
# LSTM模型定义
class RNN_model(nn.Module):
    def __init__(self, ...):
        self.rnn_lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=lstm_hidden_dim,
            num_layers=2,  # 双层LSTM
            batch_first=True
        )
        self.fc = nn.Linear(lstm_hidden_dim, vocab_len)  # 输出层

    def forward(self, x):
        batch_input = self.word_embedding_lookup(x)  # 词嵌入
        output, _ = self.rnn_lstm(batch_input)  # LSTM处理
        out = self.fc(output.contiguous().view(-1, lstm_hidden_dim))
        return self.softmax(out)  # 概率分布
```

三、训练过程（main.py）

将数据划分为64大小的batch，每个字符的预测目标为下一个字符，每2个epoch保存模型。

```python
for epoch in range(31):
    x_batches, y_batches = generate_batch(BATCH_SIZE, poems_vector)  # 生成批次
    for batch in x_batches:
        x = torch.from_numpy(x).to(device)  # 输入序列
        y = torch.from_numpy(y).to(device)  # 目标序列（右移一位）
        pre = rnn_model(x)  # 前向传播
        loss = loss_fun(pre, y)  # 计算NLLLoss
        loss.backward()  # 反向传播
        torch.nn.utils.clip_grad_norm(rnn_model.parameters(), 1)  # 梯度裁剪
        optimizer.step()
```

四、生成机制（main.py）

* 自回归生成：以起始字初始化序列，每次取当前序列预测下一个字符，将新字符追加到序列继续预测。

* 终止条件：生成结束符E或超过30字时停止，通过argmax选择概率最高的字符。

```python
# 生成核心代码（gen_poem()）
poem = begin_word
while word != end_token:
    input = [word_int_map[w] for w in poem]  # 当前序列数字化
    output = rnn_model(input, is_test=True)  # 模型推理
    word = vocabularies[np.argmax(output)]  # 选择最高概率字符
    poem += word  # 自回归生成
```



### 3.实验结果

![训练过程](D:\Undergraduate\deeplearning\第三次作业\chap6_RNN\tangshi_for_pytorch\训练过程.png)

![result](D:\Undergraduate\deeplearning\第三次作业\chap6_RNN\tangshi_for_pytorch\result.png)

